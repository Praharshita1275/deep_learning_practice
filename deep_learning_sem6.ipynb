{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfmSQ/Acc3IeUH/LDgZ8+p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praharshita1275/deep_learning_practice/blob/main/deep_learning_sem6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 1\n"
      ],
      "metadata": {
        "id": "WLfoe63yq6co"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zrhaZtntiU1i",
        "outputId": "4972b5ab-8936-4176-b005-ce573db2b9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "Successfully installed keras-3.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "eb8a9efe8f2e4979a826eef5e09a6611"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install keras --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Code: Build a Simple Neural Network with PyTorch\n",
        "import torch                          # Imports the core PyTorch library for tensor operations\n",
        "import torch.nn as nn                 # Imports neural network modules (layers, loss functions)\n",
        "import torch.optim as optim           # Imports optimization algorithms\n",
        "\n",
        "# Dummy dataset\n",
        "x = torch.randn(100, 3)               # Creates input data with 100 samples and 3 features each\n",
        "y = torch.randn(100, 1)               # Creates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Define model\n",
        "class SimpleNet(nn.Module):           # Defines a neural network class inheriting from nn.Module\n",
        "    def __init__(self):\n",
        "        super().__init__()             # Initializes the parent nn.Module class\n",
        "        self.linear = nn.Linear(3, 1)  # Defines a linear layer with 3 inputs and 1 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)          # Specifies the forward pass computation\n",
        "\n",
        "model = SimpleNet()                   # Creates an instance of the neural network\n",
        "\n",
        "loss_fn = nn.MSELoss()                # Defines Mean Squared Error as the loss function\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),               # Passes model parameters to the optimizer\n",
        "    lr=0.01                            # Sets the learning rate\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):              # Runs training for 100 epochs\n",
        "    y_pred = model(x)                 # Performs forward pass to get predictions\n",
        "    loss = loss_fn(y_pred, y)         # Computes loss between predictions and true values\n",
        "\n",
        "    optimizer.zero_grad()              # Clears previous gradients\n",
        "    loss.backward()                   # Computes gradients using backpropagation\n",
        "    optimizer.step()                  # Updates model parameters\n",
        "\n",
        "print(\"Final loss:\", loss.item())     # Prints the final training loss as a scalar value"
      ],
      "metadata": {
        "id": "ck6__NyOpL0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87763f3a-e728-4d73-92d3-16ffe1d90c9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.7382493019104004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Code: Simple Neural Network with TensorFlow\n",
        "\n",
        "import tensorflow as tf                    # Imports the TensorFlow library for deep learning\n",
        "\n",
        "# Dummy data\n",
        "x = tf.random.normal((100, 3))             # Creates input data with 100 samples and 3 features each\n",
        "y = tf.random.normal((100, 1))             # Creates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([              # Creates a Sequential neural network model\n",
        "    tf.keras.layers.Dense(1,               # Adds a Dense (fully connected) layer with 1 output neuron\n",
        "                          input_shape=(3,))# Specifies the input dimension as 3 features\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',             # Configures the Adam optimizer for training\n",
        "              loss='mse')                   # Uses Mean Squared Error as the loss function\n",
        "\n",
        "# Train model\n",
        "model.fit(x, y,                             # Trains the model using input and target data\n",
        "          epochs=100,                       # Number of training iterations over the dataset\n",
        "          verbose=0)                        # Suppresses training progress output\n",
        "\n",
        "print(\"Final loss:\",                       # Prints a message label\n",
        "      model.evaluate(x, y))                # Evaluates the trained model on the same dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8gEfFjdqxOa",
        "outputId": "afd2a3e8-9d12-437c-e766-30e3a770ff4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6081\n",
            "Final loss: 1.6080598831176758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Code: Same Network Using Keras (via tf.keras)\n",
        "\n",
        "from tensorflow import keras              # Imports Keras API from TensorFlow\n",
        "from tensorflow.keras import layers       # Imports neural network layers module\n",
        "\n",
        "# Dummy data\n",
        "x = tf.random.normal((100, 3))             # Generates input data with 100 samples and 3 features each\n",
        "y = tf.random.normal((100, 1))             # Generates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Model\n",
        "model = keras.Sequential([                # Creates a Sequential Keras model\n",
        "    layers.Dense(1,                       # Adds a Dense (fully connected) layer with 1 output neuron\n",
        "                 input_shape=(3,))        # Specifies input dimension as 3 features\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',            # Configures Adam optimizer for training\n",
        "              loss='mse')                  # Sets Mean Squared Error as the loss function\n",
        "\n",
        "model.fit(x, y,                            # Trains the model on input and target data\n",
        "          epochs=100,                      # Number of complete passes over the dataset\n",
        "          verbose=0)                       # Suppresses training progress output\n",
        "\n",
        "print(\"Final loss:\",                       # Prints a label for the output\n",
        "      model.evaluate(x, y))                # Evaluates the trained model on the same dataset\n"
      ],
      "metadata": {
        "id": "Zsm0xkclq1n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d606edb-9be1-43ec-f547-f6888095c611"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8043\n",
            "Final loss: 1.8043233156204224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPLEMENT A SIMPLE PERCEPTRON (Coding a Neuron)\n",
        "\n",
        "import numpy as np                        # Imports NumPy for numerical and vector operations\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Sigmoid activation function: squashes input values between 0 and 1\n",
        "  # Formula: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))             # Computes sigmoid of x\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights                # Stores the weights of the neuron\n",
        "    self.bias = bias                      # Stores the bias value\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    # Computes the weighted sum of inputs and bias\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    # Applies the sigmoid activation function to the total input\n",
        "    return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1])                # Defines weights: w1 = 0, w2 = 1\n",
        "bias = 4                                  # Defines bias value b = 4\n",
        "\n",
        "n = Neuron(weights, bias)                 # Creates a neuron object with given weights and bias\n",
        "\n",
        "x = np.array([2, 3])                      # Input vector: x1 = 2, x2 = 3\n",
        "print(n.feedforward(x))                   # Outputs the neuron's activated value\n"
      ],
      "metadata": {
        "id": "Sxp2WebIq344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279cc4ba-031f-4ef4-ac3b-14889c9ede1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step function for binary classification\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights            # Stores weights\n",
        "        self.bias = bias                  # Stores bias\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias  # Weighted sum\n",
        "        return step(total)                # Binary output\n",
        "\n",
        "# AND gate parameters\n",
        "weights = np.array([1, 1])               # Both inputs must be 1\n",
        "bias = -1.5                              # Threshold shift\n",
        "\n",
        "and_gate = Perceptron(weights, bias)\n",
        "\n",
        "# Testing AND gate\n",
        "print(\"AND Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", and_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "S5GyxAAfrU6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7cfdb8-aa71-465b-a61d-2ed3c3436e94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 0\n",
            "(1, 0) -> 0\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OR gate parameters\n",
        "weights = np.array([1, 1])               # Any one input activates output\n",
        "bias = -0.5                              # Lower threshold\n",
        "\n",
        "or_gate = Perceptron(weights, bias)\n",
        "\n",
        "# Testing OR gate\n",
        "print(\"\\nOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", or_gate.predict(np.array(x)))\n"
      ],
      "metadata": {
        "id": "b11hU0O1rW6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f22ba66-64ef-4d4f-a181-69aa92cd0f13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden layer weights and biases\n",
        "        self.w1 = np.array([[1, 1], [1, 1]])   # Weights for hidden neurons DEFINDED MANUALLY\n",
        "        self.b1 = np.array([-0.5, -1.5])       # Biases for hidden neurons  DEFINDED MANUALLY\n",
        "\n",
        "        # Output layer weights and bias\n",
        "        self.w2 = np.array([1, -2])            # Weights for output neuron\n",
        "        self.b2 = -0.5                         # Bias for output neuron\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Hidden layer computation\n",
        "        h = sigmoid(np.dot(self.w1, x) + self.b1)\n",
        "\n",
        "        # Output layer computation\n",
        "        output = sigmoid(np.dot(self.w2, h) + self.b2)\n",
        "\n",
        "        return 1 if output >= 0.5 else 0        # Binary classification\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Testing XOR gate\n",
        "print(\"\\nXOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "53_yey4ErZKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a6ea55-ceb3-4bc3-9321-d78ceeec1f8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 0\n",
            "(1, 0) -> 0\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_or = np.array([1, 1])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -1.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, -2])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "        h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "M7Y8h9c5rbdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbdac6c-f867-4c1a-b6c9-a339cc5ce417"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    }
  ]
}