{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvxZGTLI9oiQdSEXQARNiD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praharshita1275/deep_learning_practice/blob/main/deep_learning_sem6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 1\n"
      ],
      "metadata": {
        "id": "WLfoe63yq6co"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zrhaZtntiU1i",
        "outputId": "4972b5ab-8936-4176-b005-ce573db2b9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "Successfully installed keras-3.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "eb8a9efe8f2e4979a826eef5e09a6611"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install keras --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Code: Build a Simple Neural Network with PyTorch\n",
        "import torch                          # Imports the core PyTorch library for tensor operations\n",
        "import torch.nn as nn                 # Imports neural network modules (layers, loss functions)\n",
        "import torch.optim as optim           # Imports optimization algorithms\n",
        "\n",
        "# Dummy dataset\n",
        "x = torch.randn(100, 3)               # Creates input data with 100 samples and 3 features each\n",
        "y = torch.randn(100, 1)               # Creates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Define model\n",
        "class SimpleNet(nn.Module):           # Defines a neural network class inheriting from nn.Module\n",
        "    def __init__(self):\n",
        "        super().__init__()             # Initializes the parent nn.Module class\n",
        "        self.linear = nn.Linear(3, 1)  # Defines a linear layer with 3 inputs and 1 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)          # Specifies the forward pass computation\n",
        "\n",
        "model = SimpleNet()                   # Creates an instance of the neural network\n",
        "\n",
        "loss_fn = nn.MSELoss()                # Defines Mean Squared Error as the loss function\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),               # Passes model parameters to the optimizer\n",
        "    lr=0.01                            # Sets the learning rate\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):              # Runs training for 100 epochs\n",
        "    y_pred = model(x)                 # Performs forward pass to get predictions\n",
        "    loss = loss_fn(y_pred, y)         # Computes loss between predictions and true values\n",
        "\n",
        "    optimizer.zero_grad()              # Clears previous gradients\n",
        "    loss.backward()                   # Computes gradients using backpropagation\n",
        "    optimizer.step()                  # Updates model parameters\n",
        "\n",
        "print(\"Final loss:\", loss.item())     # Prints the final training loss as a scalar value"
      ],
      "metadata": {
        "id": "ck6__NyOpL0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87763f3a-e728-4d73-92d3-16ffe1d90c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.7382493019104004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Code: Simple Neural Network with TensorFlow\n",
        "\n",
        "import tensorflow as tf                    # Imports the TensorFlow library for deep learning\n",
        "\n",
        "# Dummy data\n",
        "x = tf.random.normal((100, 3))             # Creates input data with 100 samples and 3 features each\n",
        "y = tf.random.normal((100, 1))             # Creates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([              # Creates a Sequential neural network model\n",
        "    tf.keras.layers.Dense(1,               # Adds a Dense (fully connected) layer with 1 output neuron\n",
        "                          input_shape=(3,))# Specifies the input dimension as 3 features\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',             # Configures the Adam optimizer for training\n",
        "              loss='mse')                   # Uses Mean Squared Error as the loss function\n",
        "\n",
        "# Train model\n",
        "model.fit(x, y,                             # Trains the model using input and target data\n",
        "          epochs=100,                       # Number of training iterations over the dataset\n",
        "          verbose=0)                        # Suppresses training progress output\n",
        "\n",
        "print(\"Final loss:\",                       # Prints a message label\n",
        "      model.evaluate(x, y))                # Evaluates the trained model on the same dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8gEfFjdqxOa",
        "outputId": "afd2a3e8-9d12-437c-e766-30e3a770ff4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6081\n",
            "Final loss: 1.6080598831176758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Code: Same Network Using Keras (via tf.keras)\n",
        "\n",
        "from tensorflow import keras              # Imports Keras API from TensorFlow\n",
        "from tensorflow.keras import layers       # Imports neural network layers module\n",
        "\n",
        "# Dummy data\n",
        "x = tf.random.normal((100, 3))             # Generates input data with 100 samples and 3 features each\n",
        "y = tf.random.normal((100, 1))             # Generates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Model\n",
        "model = keras.Sequential([                # Creates a Sequential Keras model\n",
        "    layers.Dense(1,                       # Adds a Dense (fully connected) layer with 1 output neuron\n",
        "                 input_shape=(3,))        # Specifies input dimension as 3 features\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',            # Configures Adam optimizer for training\n",
        "              loss='mse')                  # Sets Mean Squared Error as the loss function\n",
        "\n",
        "model.fit(x, y,                            # Trains the model on input and target data\n",
        "          epochs=100,                      # Number of complete passes over the dataset\n",
        "          verbose=0)                       # Suppresses training progress output\n",
        "\n",
        "print(\"Final loss:\",                       # Prints a label for the output\n",
        "      model.evaluate(x, y))                # Evaluates the trained model on the same dataset\n"
      ],
      "metadata": {
        "id": "Zsm0xkclq1n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d606edb-9be1-43ec-f547-f6888095c611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8043\n",
            "Final loss: 1.8043233156204224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPLEMENT A SIMPLE PERCEPTRON (Coding a Neuron)\n",
        "\n",
        "import numpy as np                        # Imports NumPy for numerical and vector operations\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Sigmoid activation function: squashes input values between 0 and 1\n",
        "  # Formula: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))             # Computes sigmoid of x\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights                # Stores the weights of the neuron\n",
        "    self.bias = bias                      # Stores the bias value\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    # Computes the weighted sum of inputs and bias\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    # Applies the sigmoid activation function to the total input\n",
        "    return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1])                # Defines weights: w1 = 0, w2 = 1\n",
        "bias = 4                                  # Defines bias value b = 4\n",
        "\n",
        "n = Neuron(weights, bias)                 # Creates a neuron object with given weights and bias\n",
        "\n",
        "x = np.array([2, 3])                      # Input vector: x1 = 2, x2 = 3\n",
        "print(n.feedforward(x))                   # Outputs the neuron's activated value\n"
      ],
      "metadata": {
        "id": "Sxp2WebIq344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279cc4ba-031f-4ef4-ac3b-14889c9ede1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step function for binary classification\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights            # Stores weights\n",
        "        self.bias = bias                  # Stores bias\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias  # Weighted sum\n",
        "        return step(total)                # Binary output\n",
        "\n",
        "# AND gate parameters\n",
        "weights = np.array([1, 1])               # Both inputs must be 1\n",
        "bias = -1.5                              # Threshold shift\n",
        "\n",
        "and_gate = Perceptron(weights, bias)\n",
        "\n",
        "# Testing AND gate\n",
        "print(\"AND Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", and_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "S5GyxAAfrU6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7cfdb8-aa71-465b-a61d-2ed3c3436e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 0\n",
            "(1, 0) -> 0\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OR gate parameters\n",
        "weights = np.array([1, 1])               # Any one input activates output\n",
        "bias = -0.5                              # Lower threshold\n",
        "\n",
        "or_gate = Perceptron(weights, bias)\n",
        "\n",
        "# Testing OR gate\n",
        "print(\"\\nOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", or_gate.predict(np.array(x)))\n"
      ],
      "metadata": {
        "id": "b11hU0O1rW6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f22ba66-64ef-4d4f-a181-69aa92cd0f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden layer weights and biases\n",
        "        self.w1 = np.array([[1, 1], [1, 1]])   # Weights for hidden neurons DEFINDED MANUALLY\n",
        "        self.b1 = np.array([-0.5, -1.5])       # Biases for hidden neurons  DEFINDED MANUALLY\n",
        "\n",
        "        # Output layer weights and bias\n",
        "        self.w2 = np.array([1, -2])            # Weights for output neuron\n",
        "        self.b2 = -0.5                         # Bias for output neuron\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Hidden layer computation\n",
        "        h = sigmoid(np.dot(self.w1, x) + self.b1)\n",
        "\n",
        "        # Output layer computation\n",
        "        output = sigmoid(np.dot(self.w2, h) + self.b2)\n",
        "\n",
        "        return 1 if output >= 0.5 else 0        # Binary classification\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Testing XOR gate\n",
        "print(\"\\nXOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "53_yey4ErZKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a6ea55-ceb3-4bc3-9321-d78ceeec1f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 0\n",
            "(1, 0) -> 0\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_or = np.array([1, 1])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -1.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, -2])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "        h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "M7Y8h9c5rbdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbdac6c-f867-4c1a-b6c9-a339cc5ce417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 2\n",
        "\n"
      ],
      "metadata": {
        "id": "aafCu-zPtfz_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dbf4b0a",
        "outputId": "922b2a90-9cc1-495a-ee99-182051283da7"
      },
      "source": [
        "#Implement the XOR logic operation using a multi-perceptron network, and analyze how multiple perceptrons overcome the limitations of a single perceptron.\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_or = np.array([1, 1])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -1.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, -2])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "        h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the Perceptron Learning Algorithm and study the effect of weight updates on convergence for linearly separable datasets.\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function for perceptron\n",
        "def step_function(x):\n",
        "  return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "  def __init__(self, num_inputs, learning_rate=0.1, epochs=10):\n",
        "    self.weights = np.zeros(num_inputs + 1)  # +1 for bias\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def predict(self, inputs):\n",
        "    # Add bias term to inputs by prepending 1\n",
        "    summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
        "    return step_function(summation)\n",
        "\n",
        "  def train(self, training_inputs, labels):\n",
        "    print(\"\\nStarting Perceptron Training...\")\n",
        "    for epoch in range(self.epochs):\n",
        "      print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n",
        "      total_error = 0\n",
        "      for inputs, label in zip(training_inputs, labels):\n",
        "        prediction = self.predict(inputs)\n",
        "        error = label - prediction\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # Update weights and bias\n",
        "        self.weights[1:] += self.learning_rate * error * inputs\n",
        "        self.weights[0] += self.learning_rate * error # Update bias\n",
        "        # print(f\"  Inputs: {inputs}, Label: {label}, Prediction: {prediction}, Error: {error}, New Weights: {self.weights}\")\n",
        "\n",
        "      print(f\"  Epoch {epoch + 1} finished with total error: {total_error}\")\n",
        "      if total_error == 0:\n",
        "        print(\"  Perceptron converged!\")\n",
        "        break\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "training_inputs = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Labels (AND gate output)\n",
        "labels = np.array([0, 0, 0, 1])\n",
        "perceptron = Perceptron(num_inputs=2, learning_rate=0.1, epochs=10)\n",
        "perceptron.train(training_inputs, labels)\n",
        "\n",
        "print(\"\\nTesting trained Perceptron:\")\n",
        "for inputs, label in zip(training_inputs, labels):\n",
        "    prediction = perceptron.predict(inputs)\n",
        "    print(f\"  Input: {inputs}, Expected: {label}, Predicted: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRdcg2gHpW5H",
        "outputId": "f4704ca6-1364-4a38-f6f2-122f4271c687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Perceptron Training...\n",
            "\n",
            "Epoch 1/10\n",
            "  Epoch 1 finished with total error: 2\n",
            "\n",
            "Epoch 2/10\n",
            "  Epoch 2 finished with total error: 3\n",
            "\n",
            "Epoch 3/10\n",
            "  Epoch 3 finished with total error: 3\n",
            "\n",
            "Epoch 4/10\n",
            "  Epoch 4 finished with total error: 0\n",
            "  Perceptron converged!\n",
            "Training complete.\n",
            "\n",
            "Testing trained Perceptron:\n",
            "  Input: [0 0], Expected: 0, Predicted: 0\n",
            "  Input: [0 1], Expected: 0, Predicted: 0\n",
            "  Input: [1 0], Expected: 0, Predicted: 0\n",
            "  Input: [1 1], Expected: 1, Predicted: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Demonstrate the representational power of a perceptron by evaluating its capability and limitations in solving classification problems.\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_or = np.array([1, -2])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, 1])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_and, x) + self.b_or)     # AND result\n",
        "        h2 = step(np.dot(self.w_or, x) + self.b_and)  # OR result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54IiOkVIq1D4",
        "outputId": "62b37bce-f6a4-40fd-9a62-09765738e449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def step(x):\n",
        "  return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "  def __init__(self):\n",
        "    # Hidden neuron 1: OR gate\n",
        "    self.w_or = np.array([1, 1])\n",
        "    self.b_or = -0.5\n",
        "\n",
        "    # Hidden neuron 2: AND gate\n",
        "    self.w_and = np.array([1, 1])\n",
        "    self.b_and = -1.5\n",
        "\n",
        "    # Output neuron: OR - AND\n",
        "    self.w_out = np.array([1, -2])\n",
        "    self.b_out = -0.5\n",
        "\n",
        "  def predict(self, x):\n",
        "    h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "    h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "    output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "    return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gm73xRKsnqQ",
        "outputId": "136906de-c0ff-4852-ceba-3a4f065c96f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Design and implement a Multi-Layer Perceptron (MLP) architecture capable of realizing all basic Boolean functions.\n",
        "import numpy as np\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = np.array(weights)\n",
        "        self.bias = bias\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return step(total)\n",
        "\n",
        "print(\"\\nAND Gate\")\n",
        "and_perceptron = Perceptron(weights=[1, 1], bias=-1.5)\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(f\"AND{x} -> {and_perceptron.predict(np.array(x))}\")\n",
        "\n",
        "# --- Implement OR Gate ---\n",
        "print(\"\\nOR Gate\")\n",
        "or_perceptron = Perceptron(weights=[1, 1], bias=-0.5)\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(f\"OR{x} -> {or_perceptron.predict(np.array(x))}\")\n",
        "\n",
        "# --- Implement NOT Gate ---\n",
        "print(\"\\nNOT Gate\")\n",
        "not_perceptron = Perceptron(weights=[-1], bias=0.5)\n",
        "for x in [0, 1]:\n",
        "    print(f\"NOT({x}) -> {not_perceptron.predict(np.array([x]))}\")\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate like parameters\n",
        "        self.w_or_h = np.array([1, 1])\n",
        "        self.b_or_h = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate like parameters\n",
        "        self.w_and_h = np.array([1, 1])\n",
        "        self.b_and_h = -1.5\n",
        "\n",
        "        self.w_out = np.array([1, -2]) # w_out[0] for OR_h, w_out[1] for AND_h\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Hidden layer computation\n",
        "        h1_output = step(np.dot(self.w_or_h, x) + self.b_or_h)   # Output of first hidden neuron (OR-like)\n",
        "        h2_output = step(np.dot(self.w_and_h, x) + self.b_and_h) # Output of second hidden neuron (AND-like)\n",
        "\n",
        "        # Output layer computation\n",
        "        hidden_outputs = np.array([h1_output, h2_output])\n",
        "        final_output = step(np.dot(self.w_out, hidden_outputs) + self.b_out)\n",
        "        return final_output\n",
        "\n",
        "print(\"\\nXOR Gate (Multi-Layer Perceptron)\")\n",
        "xor_mlp = XOR_Network()\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(f\"XOR{x} -> {xor_mlp.predict(np.array(x))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuR0nGUztPr5",
        "outputId": "99538fce-b676-4ca2-a293-71b152f8dc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AND Gate\n",
            "AND(0, 0) -> 0\n",
            "AND(0, 1) -> 0\n",
            "AND(1, 0) -> 0\n",
            "AND(1, 1) -> 1\n",
            "\n",
            "OR Gate\n",
            "OR(0, 0) -> 0\n",
            "OR(0, 1) -> 1\n",
            "OR(1, 0) -> 1\n",
            "OR(1, 1) -> 1\n",
            "\n",
            "NOT Gate\n",
            "NOT(0) -> 1\n",
            "NOT(1) -> 0\n",
            "\n",
            "XOR Gate (Multi-Layer Perceptron)\n",
            "XOR(0, 0) -> 0\n",
            "XOR(0, 1) -> 1\n",
            "XOR(1, 0) -> 1\n",
            "XOR(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement an MLP by varying bias, weights, and learning rate, and record observations for different learning rate values.\n",
        "#Plot a graph showing the relationship between loss (error) and learning rate.\n",
        "\n",
        "import numpy as np\n",
        "def sigmoid(x):\n"
      ],
      "metadata": {
        "id": "jQZW6YLqxerB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the Perceptron Learning Algorithm and study the effect of weight updates on convergence for a binary decision problem such as\n",
        "#determining whether a user would like to watch a movie.\n",
        "import numpy as np\n",
        "\n",
        "# Input features for different movies\n",
        "X = np.array([\n",
        "    [0, 1, 0, 1],\n",
        "    [1, 0, 1, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [1, 1, 1, 1],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "\n",
        "# Corresponding labels: Would watch (1) or not (0)\n",
        "y = np.array([0, 0, 1, 1, 0]) # Example labels based on assumed preferences\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, num_inputs, learning_rate=0.1, epochs=100):\n",
        "        self.weights = np.zeros(num_inputs + 1)  # +1 for bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0] # Weighted sum + bias\n",
        "        return step(summation)\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "        print(\"\\nStarting Perceptron Training for Movie Preference...\")\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                prediction = self.predict(inputs)\n",
        "                error = label - prediction\n",
        "                total_error += abs(error)\n",
        "\n",
        "                # Update weights and bias\n",
        "                self.weights[1:] += self.learning_rate * error * inputs\n",
        "                self.weights[0] += self.learning_rate * error # Update bias\n",
        "\n",
        "            # print(f\"Epoch {epoch + 1}/{self.epochs}, Total Error: {total_error}, Weights: {self.weights}\")\n",
        "            if total_error == 0:\n",
        "                print(f\"Perceptron converged after {epoch + 1} epochs!\")\n",
        "                break\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "# Initialize and train the Perceptron\n",
        "num_features = X.shape[1]\n",
        "movie_perceptron = Perceptron(num_features, learning_rate=0.05, epochs=1000)\n",
        "movie_perceptron.train(X, y)\n",
        "\n",
        "# Test the trained Perceptron\n",
        "print(\"\\nTesting trained Movie Perceptron:\")\n",
        "for i, (inputs, label) in enumerate(zip(X, y)):\n",
        "    prediction = movie_perceptron.predict(inputs)\n",
        "    print(f\"  Movie {i+1} - Features: {inputs}, Expected: {label}, Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C_1iQwg4C4K",
        "outputId": "1cd66112-dee8-4cab-887a-f641a8408050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Perceptron Training for Movie Preference...\n",
            "Perceptron converged after 7 epochs!\n",
            "Training complete.\n",
            "\n",
            "Testing trained Movie Perceptron:\n",
            "  Movie 1 - Features: [0 1 0 1], Expected: 0, Predicted: 0\n",
            "  Movie 2 - Features: [1 0 1 0], Expected: 0, Predicted: 0\n",
            "  Movie 3 - Features: [0 1 0 0], Expected: 1, Predicted: 1\n",
            "  Movie 4 - Features: [1 1 1 1], Expected: 1, Predicted: 1\n",
            "  Movie 5 - Features: [0 0 0 1], Expected: 0, Predicted: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5EvRISor7e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "# Design and implement a Multi-Layer Perceptron (MLP) architecture capable of realizing all basic Boolean functions (i.e with linearly separable or not data)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Boolean input combinations\n",
        "# -----------------------------\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Boolean functions\n",
        "boolean_functions = {\n",
        "    \"AND\":  np.array([[0], [0], [0], [1]]),\n",
        "    \"OR\":   np.array([[0], [1], [1], [1]]),\n",
        "    \"NAND\": np.array([[1], [1], [1], [0]]),\n",
        "    \"NOR\":  np.array([[1], [0], [0], [0]]),\n",
        "    \"XOR\":  np.array([[0], [1], [1], [0]]),\n",
        "    \"XNOR\": np.array([[1], [0], [0], [1]])\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Activation functions\n",
        "# -----------------------------\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# -----------------------------\n",
        "# MLP Training Function\n",
        "# -----------------------------\n",
        "def train_mlp(X, y, hidden_neurons=2, lr=0.1, epochs=10000):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Initialize weights & biases\n",
        "    W1 = np.random.rand(2, hidden_neurons)\n",
        "    b1 = np.random.rand(1, hidden_neurons)\n",
        "    W2 = np.random.rand(hidden_neurons, 1)\n",
        "    b2 = np.random.rand(1, 1)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # ---- Forward pass ----\n",
        "        hidden_input = np.dot(X, W1) + b1\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "        final_input = np.dot(hidden_output, W2) + b2\n",
        "        y_pred = sigmoid(final_input)\n",
        "\n",
        "        # ---- Backpropagation ----\n",
        "        error = y - y_pred\n",
        "        d_output = error * sigmoid_derivative(y_pred)\n",
        "        d_hidden = d_output.dot(W2.T) * sigmoid_derivative(hidden_output)\n",
        "\n",
        "        # ---- Update weights & biases ----\n",
        "        W2 += hidden_output.T.dot(d_output) * lr\n",
        "        b2 += np.sum(d_output, axis=0, keepdims=True) * lr\n",
        "\n",
        "        W1 += X.T.dot(d_hidden) * lr\n",
        "        b1 += np.sum(d_hidden, axis=0, keepdims=True) * lr\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# -----------------------------\n",
        "# Test the MLP\n",
        "# -----------------------------\n",
        "print(\"\\nMLP Results for Boolean Functions:\\n\")\n",
        "\n",
        "for name, y in boolean_functions.items():\n",
        "    W1, b1, W2, b2 = train_mlp(X, y)\n",
        "\n",
        "    hidden = sigmoid(np.dot(X, W1) + b1)\n",
        "    output = sigmoid(np.dot(hidden, W2) + b2)\n",
        "    predictions = (output >= 0.5).astype(int)\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    for i in range(len(X)):\n",
        "        print(f\"  {X[i]} -> {predictions[i][0]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "g5k4wj0eQ8gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c75b68f-215b-4c3c-c52a-2ef3aa00871b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MLP Results for Boolean Functions:\n",
            "\n",
            "AND:\n",
            "  [0 0] -> 0\n",
            "  [0 1] -> 0\n",
            "  [1 0] -> 0\n",
            "  [1 1] -> 1\n",
            "\n",
            "OR:\n",
            "  [0 0] -> 0\n",
            "  [0 1] -> 1\n",
            "  [1 0] -> 1\n",
            "  [1 1] -> 1\n",
            "\n",
            "NAND:\n",
            "  [0 0] -> 1\n",
            "  [0 1] -> 1\n",
            "  [1 0] -> 1\n",
            "  [1 1] -> 0\n",
            "\n",
            "NOR:\n",
            "  [0 0] -> 1\n",
            "  [0 1] -> 0\n",
            "  [1 0] -> 0\n",
            "  [1 1] -> 0\n",
            "\n",
            "XOR:\n",
            "  [0 0] -> 0\n",
            "  [0 1] -> 1\n",
            "  [1 0] -> 1\n",
            "  [1 1] -> 0\n",
            "\n",
            "XNOR:\n",
            "  [0 0] -> 1\n",
            "  [0 1] -> 0\n",
            "  [1 0] -> 0\n",
            "  [1 1] -> 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8\n",
        "# How many Boolean functions can be designed using three binary inputs?\n",
        "# Find respective weights w1 to w8) and demonstrate for all the possible boolean functions for three inputs.\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# -----------------------------\n",
        "# 3-input combinations\n",
        "# -----------------------------\n",
        "X = np.array([\n",
        "    [0, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 0],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 0],\n",
        "    [1, 1, 1]\n",
        "])\n",
        "\n",
        "# Step activation\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Single perceptron training\n",
        "def train_perceptron(X, y, lr=0.1, epochs=50):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        errors = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = step(np.dot(X[i], w) + b)\n",
        "            error = y[i] - y_pred\n",
        "            if error != 0:\n",
        "                w += lr * error * X[i]\n",
        "                b += lr * error\n",
        "                errors += 1\n",
        "        if errors == 0:\n",
        "            return True   # Linearly separable\n",
        "    return False          # Not linearly separable\n",
        "\n",
        "# -----------------------------\n",
        "# Generate all Boolean functions\n",
        "# -----------------------------\n",
        "boolean_functions = list(itertools.product([0, 1], repeat=8))\n",
        "\n",
        "separable = 0\n",
        "non_separable = 0\n",
        "\n",
        "for func in boolean_functions:\n",
        "    y = np.array(func)\n",
        "    if train_perceptron(X, y):\n",
        "        separable += 1\n",
        "    else:\n",
        "        non_separable += 1\n",
        "\n",
        "# -----------------------------\n",
        "# Results\n",
        "# -----------------------------\n",
        "print(\"Total Boolean functions (3 inputs):\", len(boolean_functions))\n",
        "print(\"Linearly separable functions:\", separable)\n",
        "print(\"Non-linearly separable functions:\", non_separable)\n"
      ],
      "metadata": {
        "id": "4G1mL-ipQ9sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ffb1bb-519a-47e9-a2e7-9d9e394bac61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Boolean functions (3 inputs): 256\n",
            "Linearly separable functions: 104\n",
            "Non-linearly separable functions: 152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9) Implement an MLP by varying bias, weights, and learning rate, and record observations for different learning rate values.\n",
        "# Plot a graph showing the relationship between loss (error) and learning rate."
      ],
      "metadata": {
        "id": "5EFJe-TK4NAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}