{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa9eKq3Aacg3uxr5k6lgBG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praharshita1275/deep_learning_practice/blob/main/deep_learning_sem6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 1\n"
      ],
      "metadata": {
        "id": "WLfoe63yq6co"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zrhaZtntiU1i",
        "outputId": "4972b5ab-8936-4176-b005-ce573db2b9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "Successfully installed keras-3.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "eb8a9efe8f2e4979a826eef5e09a6611"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install keras --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Code: Build a Simple Neural Network with PyTorch\n",
        "import torch                          # Imports the core PyTorch library for tensor operations\n",
        "import torch.nn as nn                 # Imports neural network modules (layers, loss functions)\n",
        "import torch.optim as optim           # Imports optimization algorithms\n",
        "\n",
        "# Dummy dataset\n",
        "x = torch.randn(100, 3)               # Creates input data with 100 samples and 3 features each\n",
        "y = torch.randn(100, 1)               # Creates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Define model\n",
        "class SimpleNet(nn.Module):           # Defines a neural network class inheriting from nn.Module\n",
        "    def __init__(self):\n",
        "        super().__init__()             # Initializes the parent nn.Module class\n",
        "        self.linear = nn.Linear(3, 1)  # Defines a linear layer with 3 inputs and 1 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)          # Specifies the forward pass computation\n",
        "\n",
        "model = SimpleNet()                   # Creates an instance of the neural network\n",
        "\n",
        "loss_fn = nn.MSELoss()                # Defines Mean Squared Error as the loss function\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),               # Passes model parameters to the optimizer\n",
        "    lr=0.01                            # Sets the learning rate\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):              # Runs training for 100 epochs\n",
        "    y_pred = model(x)                 # Performs forward pass to get predictions\n",
        "    loss = loss_fn(y_pred, y)         # Computes loss between predictions and true values\n",
        "\n",
        "    optimizer.zero_grad()              # Clears previous gradients\n",
        "    loss.backward()                   # Computes gradients using backpropagation\n",
        "    optimizer.step()                  # Updates model parameters\n",
        "\n",
        "print(\"Final loss:\", loss.item())     # Prints the final training loss as a scalar value"
      ],
      "metadata": {
        "id": "ck6__NyOpL0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87763f3a-e728-4d73-92d3-16ffe1d90c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.7382493019104004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Code: Simple Neural Network with TensorFlow\n",
        "\n",
        "import tensorflow as tf                    # Imports the TensorFlow library for deep learning\n",
        "\n",
        "# Dummy data\n",
        "x = tf.random.normal((100, 3))             # Creates input data with 100 samples and 3 features each\n",
        "y = tf.random.normal((100, 1))             # Creates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([              # Creates a Sequential neural network model\n",
        "    tf.keras.layers.Dense(1,               # Adds a Dense (fully connected) layer with 1 output neuron\n",
        "                          input_shape=(3,))# Specifies the input dimension as 3 features\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',             # Configures the Adam optimizer for training\n",
        "              loss='mse')                   # Uses Mean Squared Error as the loss function\n",
        "\n",
        "# Train model\n",
        "model.fit(x, y,                             # Trains the model using input and target data\n",
        "          epochs=100,                       # Number of training iterations over the dataset\n",
        "          verbose=0)                        # Suppresses training progress output\n",
        "\n",
        "print(\"Final loss:\",                       # Prints a message label\n",
        "      model.evaluate(x, y))                # Evaluates the trained model on the same dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8gEfFjdqxOa",
        "outputId": "afd2a3e8-9d12-437c-e766-30e3a770ff4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6081\n",
            "Final loss: 1.6080598831176758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Code: Same Network Using Keras (via tf.keras)\n",
        "\n",
        "from tensorflow import keras              # Imports Keras API from TensorFlow\n",
        "from tensorflow.keras import layers       # Imports neural network layers module\n",
        "\n",
        "# Dummy data\n",
        "x = tf.random.normal((100, 3))             # Generates input data with 100 samples and 3 features each\n",
        "y = tf.random.normal((100, 1))             # Generates target output data with 100 samples and 1 value each\n",
        "\n",
        "# Model\n",
        "model = keras.Sequential([                # Creates a Sequential Keras model\n",
        "    layers.Dense(1,                       # Adds a Dense (fully connected) layer with 1 output neuron\n",
        "                 input_shape=(3,))        # Specifies input dimension as 3 features\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',            # Configures Adam optimizer for training\n",
        "              loss='mse')                  # Sets Mean Squared Error as the loss function\n",
        "\n",
        "model.fit(x, y,                            # Trains the model on input and target data\n",
        "          epochs=100,                      # Number of complete passes over the dataset\n",
        "          verbose=0)                       # Suppresses training progress output\n",
        "\n",
        "print(\"Final loss:\",                       # Prints a label for the output\n",
        "      model.evaluate(x, y))                # Evaluates the trained model on the same dataset\n"
      ],
      "metadata": {
        "id": "Zsm0xkclq1n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d606edb-9be1-43ec-f547-f6888095c611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8043\n",
            "Final loss: 1.8043233156204224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPLEMENT A SIMPLE PERCEPTRON (Coding a Neuron)\n",
        "\n",
        "import numpy as np                        # Imports NumPy for numerical and vector operations\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Sigmoid activation function: squashes input values between 0 and 1\n",
        "  # Formula: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))             # Computes sigmoid of x\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights                # Stores the weights of the neuron\n",
        "    self.bias = bias                      # Stores the bias value\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    # Computes the weighted sum of inputs and bias\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    # Applies the sigmoid activation function to the total input\n",
        "    return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1])                # Defines weights: w1 = 0, w2 = 1\n",
        "bias = 4                                  # Defines bias value b = 4\n",
        "\n",
        "n = Neuron(weights, bias)                 # Creates a neuron object with given weights and bias\n",
        "\n",
        "x = np.array([2, 3])                      # Input vector: x1 = 2, x2 = 3\n",
        "print(n.feedforward(x))                   # Outputs the neuron's activated value\n"
      ],
      "metadata": {
        "id": "Sxp2WebIq344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279cc4ba-031f-4ef4-ac3b-14889c9ede1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step function for binary classification\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights            # Stores weights\n",
        "        self.bias = bias                  # Stores bias\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias  # Weighted sum\n",
        "        return step(total)                # Binary output\n",
        "\n",
        "# AND gate parameters\n",
        "weights = np.array([1, 1])               # Both inputs must be 1\n",
        "bias = -1.5                              # Threshold shift\n",
        "\n",
        "and_gate = Perceptron(weights, bias)\n",
        "\n",
        "# Testing AND gate\n",
        "print(\"AND Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", and_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "S5GyxAAfrU6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7cfdb8-aa71-465b-a61d-2ed3c3436e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 0\n",
            "(1, 0) -> 0\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OR gate parameters\n",
        "weights = np.array([1, 1])               # Any one input activates output\n",
        "bias = -0.5                              # Lower threshold\n",
        "\n",
        "or_gate = Perceptron(weights, bias)\n",
        "\n",
        "# Testing OR gate\n",
        "print(\"\\nOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", or_gate.predict(np.array(x)))\n"
      ],
      "metadata": {
        "id": "b11hU0O1rW6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f22ba66-64ef-4d4f-a181-69aa92cd0f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden layer weights and biases\n",
        "        self.w1 = np.array([[1, 1], [1, 1]])   # Weights for hidden neurons DEFINDED MANUALLY\n",
        "        self.b1 = np.array([-0.5, -1.5])       # Biases for hidden neurons  DEFINDED MANUALLY\n",
        "\n",
        "        # Output layer weights and bias\n",
        "        self.w2 = np.array([1, -2])            # Weights for output neuron\n",
        "        self.b2 = -0.5                         # Bias for output neuron\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Hidden layer computation\n",
        "        h = sigmoid(np.dot(self.w1, x) + self.b1)\n",
        "\n",
        "        # Output layer computation\n",
        "        output = sigmoid(np.dot(self.w2, h) + self.b2)\n",
        "\n",
        "        return 1 if output >= 0.5 else 0        # Binary classification\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Testing XOR gate\n",
        "print(\"\\nXOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "53_yey4ErZKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a6ea55-ceb3-4bc3-9321-d78ceeec1f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 0\n",
            "(1, 0) -> 0\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_or = np.array([1, 1])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -1.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, -2])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "        h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "id": "M7Y8h9c5rbdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbdac6c-f867-4c1a-b6c9-a339cc5ce417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 2\n",
        "\n"
      ],
      "metadata": {
        "id": "aafCu-zPtfz_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dbf4b0a",
        "outputId": "922b2a90-9cc1-495a-ee99-182051283da7"
      },
      "source": [
        "#Implement the XOR logic operation using a multi-perceptron network, and analyze how multiple perceptrons overcome the limitations of a single perceptron.\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_or = np.array([1, 1])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -1.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, -2])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "        h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the Perceptron Learning Algorithm and study the effect of weight updates on convergence for linearly separable datasets.\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function for perceptron\n",
        "def step_function(x):\n",
        "  return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "  def __init__(self, num_inputs, learning_rate=0.1, epochs=10):\n",
        "    self.weights = np.zeros(num_inputs + 1)  # +1 for bias\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def predict(self, inputs):\n",
        "    # Add bias term to inputs by prepending 1\n",
        "    summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
        "    return step_function(summation)\n",
        "\n",
        "  def train(self, training_inputs, labels):\n",
        "    print(\"\\nStarting Perceptron Training...\")\n",
        "    for epoch in range(self.epochs):\n",
        "      print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n",
        "      total_error = 0\n",
        "      for inputs, label in zip(training_inputs, labels):\n",
        "        prediction = self.predict(inputs)\n",
        "        error = label - prediction\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # Update weights and bias\n",
        "        self.weights[1:] += self.learning_rate * error * inputs\n",
        "        self.weights[0] += self.learning_rate * error # Update bias\n",
        "        # print(f\"  Inputs: {inputs}, Label: {label}, Prediction: {prediction}, Error: {error}, New Weights: {self.weights}\")\n",
        "\n",
        "      print(f\"  Epoch {epoch + 1} finished with total error: {total_error}\")\n",
        "      if total_error == 0:\n",
        "        print(\"  Perceptron converged!\")\n",
        "        break\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "training_inputs = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Labels (AND gate output)\n",
        "labels = np.array([0, 0, 0, 1])\n",
        "perceptron = Perceptron(num_inputs=2, learning_rate=0.1, epochs=10)\n",
        "perceptron.train(training_inputs, labels)\n",
        "\n",
        "print(\"\\nTesting trained Perceptron:\")\n",
        "for inputs, label in zip(training_inputs, labels):\n",
        "    prediction = perceptron.predict(inputs)\n",
        "    print(f\"  Input: {inputs}, Expected: {label}, Predicted: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRdcg2gHpW5H",
        "outputId": "f4704ca6-1364-4a38-f6f2-122f4271c687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Perceptron Training...\n",
            "\n",
            "Epoch 1/10\n",
            "  Epoch 1 finished with total error: 2\n",
            "\n",
            "Epoch 2/10\n",
            "  Epoch 2 finished with total error: 3\n",
            "\n",
            "Epoch 3/10\n",
            "  Epoch 3 finished with total error: 3\n",
            "\n",
            "Epoch 4/10\n",
            "  Epoch 4 finished with total error: 0\n",
            "  Perceptron converged!\n",
            "Training complete.\n",
            "\n",
            "Testing trained Perceptron:\n",
            "  Input: [0 0], Expected: 0, Predicted: 0\n",
            "  Input: [0 1], Expected: 0, Predicted: 0\n",
            "  Input: [1 0], Expected: 0, Predicted: 0\n",
            "  Input: [1 1], Expected: 1, Predicted: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Demonstrate the representational power of a perceptron by evaluating its capability and limitations in solving classification problems.\n",
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate\n",
        "        self.w_and = np.array([1, 1])\n",
        "        self.b_and = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate\n",
        "        self.w_or = np.array([1, -2])\n",
        "        self.b_or = -0.5\n",
        "\n",
        "        # Output neuron: OR - AND\n",
        "        self.w_out = np.array([1, 1])\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        h1 = step(np.dot(self.w_and, x) + self.b_or)     # AND result\n",
        "        h2 = step(np.dot(self.w_or, x) + self.b_and)  # OR result\n",
        "\n",
        "        output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "        return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54IiOkVIq1D4",
        "outputId": "62b37bce-f6a4-40fd-9a62-09765738e449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def step(x):\n",
        "  return 1 if x >= 0 else 0\n",
        "\n",
        "class XOR_Network:\n",
        "  def __init__(self):\n",
        "    # Hidden neuron 1: OR gate\n",
        "    self.w_or = np.array([1, 1])\n",
        "    self.b_or = -0.5\n",
        "\n",
        "    # Hidden neuron 2: AND gate\n",
        "    self.w_and = np.array([1, 1])\n",
        "    self.b_and = -1.5\n",
        "\n",
        "    # Output neuron: OR - AND\n",
        "    self.w_out = np.array([1, -2])\n",
        "    self.b_out = -0.5\n",
        "\n",
        "  def predict(self, x):\n",
        "    h1 = step(np.dot(self.w_or, x) + self.b_or)     # OR result\n",
        "    h2 = step(np.dot(self.w_and, x) + self.b_and)  # AND result\n",
        "\n",
        "    output = step(self.w_out[0]*h1 + self.w_out[1]*h2 + self.b_out)\n",
        "    return output\n",
        "\n",
        "xor_gate = XOR_Network()\n",
        "\n",
        "# Test XOR gate\n",
        "print(\"XOR Gate\")\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(x, \"->\", xor_gate.predict(np.array(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gm73xRKsnqQ",
        "outputId": "136906de-c0ff-4852-ceba-3a4f065c96f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate\n",
            "(0, 0) -> 0\n",
            "(0, 1) -> 1\n",
            "(1, 0) -> 1\n",
            "(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Design and implement a Multi-Layer Perceptron (MLP) architecture capable of realizing all basic Boolean functions.\n",
        "import numpy as np\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = np.array(weights)\n",
        "        self.bias = bias\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return step(total)\n",
        "\n",
        "print(\"\\nAND Gate\")\n",
        "and_perceptron = Perceptron(weights=[1, 1], bias=-1.5)\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(f\"AND{x} -> {and_perceptron.predict(np.array(x))}\")\n",
        "\n",
        "# --- Implement OR Gate ---\n",
        "print(\"\\nOR Gate\")\n",
        "or_perceptron = Perceptron(weights=[1, 1], bias=-0.5)\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(f\"OR{x} -> {or_perceptron.predict(np.array(x))}\")\n",
        "\n",
        "# --- Implement NOT Gate ---\n",
        "print(\"\\nNOT Gate\")\n",
        "not_perceptron = Perceptron(weights=[-1], bias=0.5)\n",
        "for x in [0, 1]:\n",
        "    print(f\"NOT({x}) -> {not_perceptron.predict(np.array([x]))}\")\n",
        "\n",
        "class XOR_Network:\n",
        "    def __init__(self):\n",
        "        # Hidden neuron 1: OR gate like parameters\n",
        "        self.w_or_h = np.array([1, 1])\n",
        "        self.b_or_h = -0.5\n",
        "\n",
        "        # Hidden neuron 2: AND gate like parameters\n",
        "        self.w_and_h = np.array([1, 1])\n",
        "        self.b_and_h = -1.5\n",
        "\n",
        "        self.w_out = np.array([1, -2]) # w_out[0] for OR_h, w_out[1] for AND_h\n",
        "        self.b_out = -0.5\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Hidden layer computation\n",
        "        h1_output = step(np.dot(self.w_or_h, x) + self.b_or_h)   # Output of first hidden neuron (OR-like)\n",
        "        h2_output = step(np.dot(self.w_and_h, x) + self.b_and_h) # Output of second hidden neuron (AND-like)\n",
        "\n",
        "        # Output layer computation\n",
        "        hidden_outputs = np.array([h1_output, h2_output])\n",
        "        final_output = step(np.dot(self.w_out, hidden_outputs) + self.b_out)\n",
        "        return final_output\n",
        "\n",
        "print(\"\\nXOR Gate (Multi-Layer Perceptron)\")\n",
        "xor_mlp = XOR_Network()\n",
        "for x in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "    print(f\"XOR{x} -> {xor_mlp.predict(np.array(x))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuR0nGUztPr5",
        "outputId": "99538fce-b676-4ca2-a293-71b152f8dc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AND Gate\n",
            "AND(0, 0) -> 0\n",
            "AND(0, 1) -> 0\n",
            "AND(1, 0) -> 0\n",
            "AND(1, 1) -> 1\n",
            "\n",
            "OR Gate\n",
            "OR(0, 0) -> 0\n",
            "OR(0, 1) -> 1\n",
            "OR(1, 0) -> 1\n",
            "OR(1, 1) -> 1\n",
            "\n",
            "NOT Gate\n",
            "NOT(0) -> 1\n",
            "NOT(1) -> 0\n",
            "\n",
            "XOR Gate (Multi-Layer Perceptron)\n",
            "XOR(0, 0) -> 0\n",
            "XOR(0, 1) -> 1\n",
            "XOR(1, 0) -> 1\n",
            "XOR(1, 1) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5EvRISor7e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2\n",
        "# Examine the feasibility of implementing the XOR operation (Non linear data) using a single perceptron.\n",
        "# If not possible, clearly explain the reason based on the concept of linear separability.\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    act = w1*x1 + w2*x2 + b\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y = np.array([0, 1, 1, 0])  # XOR outputs\n",
        "\n",
        "w1, w2, b = 1, 1, -1\n",
        "print(\"x1 x2 predicted expected\")\n",
        "for i in range(len(X)):\n",
        "    x1, x2 = X[i]\n",
        "    print(x1, x2,\n",
        "          perceptron(x1, x2, w1, w2, b),\n",
        "          y[i])\n",
        "\n",
        "  # The XOR operation cannot be implemented using a single perceptron because its input–output patterns are not linearly separable.\n",
        "  # A single perceptron can only form a linear decision boundary, whereas XOR requires a non-linear boundary. Hence, XOR requires a multi-layer perceptron."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-OD7eCOoiHt",
        "outputId": "f2fb0769-e54e-4b81-d6b3-5f1046ee184d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 x2 predicted expected\n",
            "0 0 0 0\n",
            "0 1 1 1\n",
            "1 0 1 1\n",
            "1 1 1 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the feasibility of implementing the XNOR operation (Non linear data) using a single perceptron.\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    act = w1*x1 + w2*x2 + b\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "# XNOR dataset\n",
        "X = [\n",
        "    (0, 0),\n",
        "    (0, 1),\n",
        "    (1, 0),\n",
        "    (1, 1)\n",
        "]\n",
        "y_xnor = [1, 0, 0, 1]   #XNOR outputs\n",
        "\n",
        "w1, w2, b = 1, 1, -1\n",
        "print(\"x1 x2 predicted expected\")\n",
        "for i in range(len(X)):\n",
        "    x1, x2 = X[i]\n",
        "    print(x1, x2,\n",
        "          perceptron(x1, x2, w1, w2, b),\n",
        "          y_xnor[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO9OTEvmsOtd",
        "outputId": "a1411eee-542d-4faa-feec-4b2e279f275d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 x2 predicted expected\n",
            "0 0 0 1\n",
            "0 1 1 0\n",
            "1 0 1 0\n",
            "1 1 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3\n",
        "# Implement the XOR logic operation using a multi-perceptron network, and analyze how multiple perceptrons overcome the limitations of a single perceptron.\n",
        "\n",
        "def AND(x1, x2):\n",
        "  w1=1\n",
        "  w2=1\n",
        "  bias=-1.5\n",
        "  act=w1*x1+w2*x2+bias\n",
        "  if act>=0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def OR(x1, x2):\n",
        "  w1=1\n",
        "  w2=1\n",
        "  bias=-0.5\n",
        "  act=w1*x1+w2*x2+bias\n",
        "  if act>=0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def NAND(x1, x2):\n",
        "  w1=-1\n",
        "  w2=-1\n",
        "  bias=1.5\n",
        "  act=w1*x1+w2*x2+bias\n",
        "  if act>=0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def XOR(x1, x2):\n",
        "  flo=NAND(x1, x2)\n",
        "  slo=OR(x1, x2)\n",
        "  out=AND(flo, slo)\n",
        "  return out\n",
        "\n",
        "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "print(\"x1 x2 output\")\n",
        "for x1, x2 in inputs:\n",
        "    print(x1, x2, XOR(x1, x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo406SXep-q9",
        "outputId": "8af7bbc9-f453-4448-edaa-7accab2bf126"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 x2 output\n",
            "0 0 0\n",
            "0 1 1\n",
            "1 0 1\n",
            "1 1 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the XNOR logic operation using a multi-perceptron network, and analyze how multiple perceptrons overcome the limitations of a single perceptron.\n",
        "\n",
        "def AND(x1, x2):\n",
        "    w1 = 1\n",
        "    w2 = 1\n",
        "    bias = -1.5\n",
        "    act = w1*x1 + w2*x2 + bias\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "def OR(x1, x2):\n",
        "    w1 = 1\n",
        "    w2 = 1\n",
        "    bias = -0.5\n",
        "    act = w1*x1 + w2*x2 + bias\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "def NAND(x1, x2):\n",
        "    w1 = -1\n",
        "    w2 = -1\n",
        "    bias = 1.5\n",
        "    act = w1*x1 + w2*x2 + bias\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "def XOR(x1, x2):\n",
        "    flo = NAND(x1, x2)\n",
        "    slo = OR(x1, x2)\n",
        "    return AND(flo, slo)\n",
        "\n",
        "def NOT(x):\n",
        "    w = -1\n",
        "    bias = 0.5\n",
        "    act = w*x + bias\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "def XNOR(x1, x2):\n",
        "    return NOT(XOR(x1, x2))\n",
        "\n",
        "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "print(\"x1 x2 output\")\n",
        "for x1, x2 in inputs:\n",
        "    print(x1, x2, XNOR(x1, x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EDqs8AAsDT0",
        "outputId": "8ec07b03-5c68-41c5-fa4c-793b0982af38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 x2 output\n",
            "0 0 1\n",
            "0 1 0\n",
            "1 0 0\n",
            "1 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4\n",
        "# Demonstrate that the thresholding logic used by perceptron is very harsh.\n",
        "\n",
        "def perceptron(act):\n",
        "    return 1 if act >= 0 else 0\n",
        "\n",
        "activations = [-0.01, -0.001, 0, 0.001, 0.01]\n",
        "print(\"Activation Output\")\n",
        "for a in activations:\n",
        "    print(a, \"        \", perceptron(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_VwhOXIrntc",
        "outputId": "7f002d53-8a12-4445-e19c-439b5c262b46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation Output\n",
            "-0.01          0\n",
            "-0.001          0\n",
            "0          1\n",
            "0.001          1\n",
            "0.01          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5\n",
        "# Implement the Perceptron Learning Algorithm and study the effect of weight updates on convergence for a binary decision problem such as\n",
        "# determining whether a user would like to watch a movie.\n",
        "\n",
        "# Perceptron Learning Algorithm for Movie Preference Classification (creating binary classifier)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Create Dataset\n",
        "# -----------------------------\n",
        "data = {\n",
        "    \"Matt_Damon\": [1, 1, 0, 0, 1, 0],\n",
        "    \"Thriller\":   [1, 0, 1, 0, 1, 0],\n",
        "    \"Nolan\":      [0, 1, 1, 0, 1, 0],\n",
        "    \"IMDB\":       [0.9, 0.8, 0.85, 0.4, 0.95, 0.3],\n",
        "    \"Like\":       [1, 1, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "X = df[[\"Matt_Damon\", \"Thriller\", \"Nolan\", \"IMDB\"]].values\n",
        "y = df[\"Like\"].values.reshape(-1, 1)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Activation Functions\n",
        "# -----------------------------\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Evaluation Metrics\n",
        "# -----------------------------\n",
        "def metrics(y_true, y_pred):\n",
        "    y_pred = y_pred.flatten()\n",
        "    y_true = y_true.flatten()\n",
        "\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    accuracy = (TP + TN) / len(y_true)\n",
        "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "# -----------------------------\n",
        "# 4. (i) MP Perceptron (Rule-based)\n",
        "# -----------------------------\n",
        "def mp_perceptron(x):\n",
        "    return 1 if (x[0] == 1 or x[3] > 0.7) else 0\n",
        "\n",
        "mp_preds = np.array([mp_perceptron(x) for x in X]).reshape(-1, 1)\n",
        "mp_acc, mp_prec, mp_rec = metrics(y, mp_preds)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. (ii & iii) Perceptron using Gradient Descent\n",
        "#     (Single-layer neural network)\n",
        "# -----------------------------\n",
        "np.random.seed(42)\n",
        "W = np.random.rand(X.shape[1], 1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 5000\n",
        "\n",
        "# Training loop (Gradient Descent)\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # ---- Forward propagation ----\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = sigmoid(z)\n",
        "\n",
        "    # ---- Loss (MSE) ----\n",
        "    error = y - y_pred\n",
        "    loss = np.mean(error ** 2)\n",
        "\n",
        "    # ---- Backpropagation ----\n",
        "    d_output = error * sigmoid_derivative(y_pred)\n",
        "\n",
        "    # ---- Weight & bias updates ----\n",
        "    W += learning_rate * np.dot(X.T, d_output)\n",
        "    b += learning_rate * np.sum(d_output)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Predictions & Metrics\n",
        "# -----------------------------\n",
        "final_output = sigmoid(np.dot(X, W) + b)\n",
        "final_predictions = (final_output >= 0.5).astype(int)\n",
        "\n",
        "acc, prec, rec = metrics(y, final_predictions)\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Print Metrics\n",
        "# -----------------------------\n",
        "print(\"\\n--- Model Evaluation Metrics ---\\n\")\n",
        "\n",
        "print(\"MP Perceptron (No Learning)\")\n",
        "print(f\"Accuracy : {mp_acc:.2f}\")\n",
        "print(f\"Precision: {mp_prec:.2f}\")\n",
        "print(f\"Recall   : {mp_rec:.2f}\\n\")\n",
        "\n",
        "print(\"Gradient Descent Perceptron (Weights + Bias)\")\n",
        "print(f\"Accuracy : {acc:.2f}\")\n",
        "print(f\"Precision: {prec:.2f}\")\n",
        "print(f\"Recall   : {rec:.2f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Test with New Movie\n",
        "# -----------------------------\n",
        "test_movie = np.array([[1, 1, 0, 0.88]])\n",
        "test_output = sigmoid(np.dot(test_movie, W) + b)\n",
        "test_prediction = 1 if test_output >= 0.5 else 0\n",
        "\n",
        "print(\"\\n--- Test Movie Prediction ---\")\n",
        "print(\"MP Perceptron:\", \"Like\" if mp_perceptron(test_movie[0]) else \"Dislike\")\n",
        "print(\"Gradient Descent Perceptron:\", \"Like\" if test_prediction else \"Dislike\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rvYvJs5fSs",
        "outputId": "8c71d0c6-9c59-49b3-f043-0f148709c69e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.1240\n",
            "Epoch 1000, Loss: 0.0041\n",
            "Epoch 2000, Loss: 0.0019\n",
            "Epoch 3000, Loss: 0.0013\n",
            "Epoch 4000, Loss: 0.0009\n",
            "\n",
            "--- Model Evaluation Metrics ---\n",
            "\n",
            "MP Perceptron (No Learning)\n",
            "Accuracy : 1.00\n",
            "Precision: 1.00\n",
            "Recall   : 1.00\n",
            "\n",
            "Gradient Descent Perceptron (Weights + Bias)\n",
            "Accuracy : 1.00\n",
            "Precision: 1.00\n",
            "Recall   : 1.00\n",
            "\n",
            "--- Test Movie Prediction ---\n",
            "MP Perceptron: Like\n",
            "Gradient Descent Perceptron: Like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "# Demonstrate the Representation Power of a Network of Perceptrons\n",
        "# a) How many Boolean functions can be designed using two binary inputs?\n",
        "# b) For each Boolean function, determine whether it is linearly separable.\n",
        "# c) mplement a single perceptron model and test whether it can correctly learn each Boolean function. (Mention how many it can't learn and why)\n",
        "# d) Extend the program to estimate or analyze how the number of non-linearly separable Boolean functions increases as the number of inputs n grows.\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "# -----------------------------\n",
        "# Binary input combinations (2 inputs)\n",
        "# -----------------------------\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Step activation\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# -----------------------------\n",
        "# Single Perceptron Training\n",
        "# -----------------------------\n",
        "def train_perceptron(X, y, lr=0.1, epochs=50):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        errors = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = step(np.dot(X[i], w) + b)\n",
        "            error = y[i] - y_pred\n",
        "            if error != 0:\n",
        "                w += lr * error * X[i]\n",
        "                b += lr * error\n",
        "                errors += 1\n",
        "        if errors == 0:\n",
        "            return True   # Converged (linearly separable)\n",
        "    return False          # Not converged (not linearly separable)\n",
        "\n",
        "# -----------------------------\n",
        "# (a) Total Boolean Functions\n",
        "# -----------------------------\n",
        "n = 2\n",
        "total_functions = 2 ** (2 ** n)\n",
        "print(\"Total Boolean functions (n=2):\", total_functions)\n",
        "\n",
        "# -----------------------------\n",
        "# (b) Generate all Boolean functions\n",
        "# -----------------------------\n",
        "boolean_functions = list(itertools.product([0, 1], repeat=4))\n",
        "\n",
        "linearly_separable = 0\n",
        "not_separable = 0\n",
        "\n",
        "print(\"\\nTesting Boolean Functions:\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# (c) Test each Boolean function\n",
        "# -----------------------------\n",
        "for idx, func in enumerate(boolean_functions):\n",
        "    y = np.array(func)\n",
        "    learned = train_perceptron(X, y)\n",
        "\n",
        "    if learned:\n",
        "        linearly_separable += 1\n",
        "        result = \"Linearly Separable\"\n",
        "    else:\n",
        "        not_separable += 1\n",
        "        result = \"Not Linearly Separable\"\n",
        "\n",
        "    print(f\"Function {idx+1}: {func} --> {result}\")\n",
        "\n",
        "print(\"\\nSummary for n = 2\")\n",
        "print(\"Linearly separable functions:\", linearly_separable)\n",
        "print(\"Not linearly separable functions:\", not_separable)\n",
        "\n",
        "# -----------------------------\n",
        "# (d) Growth of Non-linearly Separable Functions\n",
        "# -----------------------------\n",
        "print(\"\\nGrowth Analysis:\")\n",
        "for n in range(1, 6):\n",
        "    total = 2 ** (2 ** n)\n",
        "    print(f\"Inputs = {n} | Total Boolean Functions = {total}\")\n"
      ],
      "metadata": {
        "id": "euQ_4v4G8NY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40907200-ab01-45fc-9f29-c94680de9956"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Boolean functions (n=2): 16\n",
            "\n",
            "Testing Boolean Functions:\n",
            "\n",
            "Function 1: (0, 0, 0, 0) --> Linearly Separable\n",
            "Function 2: (0, 0, 0, 1) --> Linearly Separable\n",
            "Function 3: (0, 0, 1, 0) --> Linearly Separable\n",
            "Function 4: (0, 0, 1, 1) --> Linearly Separable\n",
            "Function 5: (0, 1, 0, 0) --> Linearly Separable\n",
            "Function 6: (0, 1, 0, 1) --> Linearly Separable\n",
            "Function 7: (0, 1, 1, 0) --> Not Linearly Separable\n",
            "Function 8: (0, 1, 1, 1) --> Linearly Separable\n",
            "Function 9: (1, 0, 0, 0) --> Linearly Separable\n",
            "Function 10: (1, 0, 0, 1) --> Not Linearly Separable\n",
            "Function 11: (1, 0, 1, 0) --> Linearly Separable\n",
            "Function 12: (1, 0, 1, 1) --> Linearly Separable\n",
            "Function 13: (1, 1, 0, 0) --> Linearly Separable\n",
            "Function 14: (1, 1, 0, 1) --> Linearly Separable\n",
            "Function 15: (1, 1, 1, 0) --> Linearly Separable\n",
            "Function 16: (1, 1, 1, 1) --> Linearly Separable\n",
            "\n",
            "Summary for n = 2\n",
            "Linearly separable functions: 14\n",
            "Not linearly separable functions: 2\n",
            "\n",
            "Growth Analysis:\n",
            "Inputs = 1 | Total Boolean Functions = 4\n",
            "Inputs = 2 | Total Boolean Functions = 16\n",
            "Inputs = 3 | Total Boolean Functions = 256\n",
            "Inputs = 4 | Total Boolean Functions = 65536\n",
            "Inputs = 5 | Total Boolean Functions = 4294967296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "# Design and implement a Multi-Layer Perceptron (MLP) architecture capable of realizing all basic Boolean functions (i.e with linearly separable or not data)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Boolean input combinations\n",
        "# -----------------------------\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Boolean functions\n",
        "boolean_functions = {\n",
        "    \"AND\":  np.array([[0], [0], [0], [1]]),\n",
        "    \"OR\":   np.array([[0], [1], [1], [1]]),\n",
        "    \"NAND\": np.array([[1], [1], [1], [0]]),\n",
        "    \"NOR\":  np.array([[1], [0], [0], [0]]),\n",
        "    \"XOR\":  np.array([[0], [1], [1], [0]]),\n",
        "    \"XNOR\": np.array([[1], [0], [0], [1]])\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Activation functions\n",
        "# -----------------------------\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# -----------------------------\n",
        "# MLP Training Function\n",
        "# -----------------------------\n",
        "def train_mlp(X, y, hidden_neurons=2, lr=0.1, epochs=10000):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Initialize weights & biases\n",
        "    W1 = np.random.rand(2, hidden_neurons)\n",
        "    b1 = np.random.rand(1, hidden_neurons)\n",
        "    W2 = np.random.rand(hidden_neurons, 1)\n",
        "    b2 = np.random.rand(1, 1)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # ---- Forward pass ----\n",
        "        hidden_input = np.dot(X, W1) + b1\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "        final_input = np.dot(hidden_output, W2) + b2\n",
        "        y_pred = sigmoid(final_input)\n",
        "\n",
        "        # ---- Backpropagation ----\n",
        "        error = y - y_pred\n",
        "        d_output = error * sigmoid_derivative(y_pred)\n",
        "        d_hidden = d_output.dot(W2.T) * sigmoid_derivative(hidden_output)\n",
        "\n",
        "        # ---- Update weights & biases ----\n",
        "        W2 += hidden_output.T.dot(d_output) * lr\n",
        "        b2 += np.sum(d_output, axis=0, keepdims=True) * lr\n",
        "\n",
        "        W1 += X.T.dot(d_hidden) * lr\n",
        "        b1 += np.sum(d_hidden, axis=0, keepdims=True) * lr\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# -----------------------------\n",
        "# Test the MLP\n",
        "# -----------------------------\n",
        "print(\"\\nMLP Results for Boolean Functions:\\n\")\n",
        "\n",
        "for name, y in boolean_functions.items():\n",
        "    W1, b1, W2, b2 = train_mlp(X, y)\n",
        "\n",
        "    hidden = sigmoid(np.dot(X, W1) + b1)\n",
        "    output = sigmoid(np.dot(hidden, W2) + b2)\n",
        "    predictions = (output >= 0.5).astype(int)\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    for i in range(len(X)):\n",
        "        print(f\"  {X[i]} -> {predictions[i][0]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "g5k4wj0eQ8gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a867f5c-3ced-4b40-afc6-f70cbb52f552"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MLP Results for Boolean Functions:\n",
            "\n",
            "AND:\n",
            "  [0 0] -> 0\n",
            "  [0 1] -> 0\n",
            "  [1 0] -> 0\n",
            "  [1 1] -> 1\n",
            "\n",
            "OR:\n",
            "  [0 0] -> 0\n",
            "  [0 1] -> 1\n",
            "  [1 0] -> 1\n",
            "  [1 1] -> 1\n",
            "\n",
            "NAND:\n",
            "  [0 0] -> 1\n",
            "  [0 1] -> 1\n",
            "  [1 0] -> 1\n",
            "  [1 1] -> 0\n",
            "\n",
            "NOR:\n",
            "  [0 0] -> 1\n",
            "  [0 1] -> 0\n",
            "  [1 0] -> 0\n",
            "  [1 1] -> 0\n",
            "\n",
            "XOR:\n",
            "  [0 0] -> 0\n",
            "  [0 1] -> 1\n",
            "  [1 0] -> 1\n",
            "  [1 1] -> 0\n",
            "\n",
            "XNOR:\n",
            "  [0 0] -> 1\n",
            "  [0 1] -> 0\n",
            "  [1 0] -> 0\n",
            "  [1 1] -> 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8\n",
        "# How many Boolean functions can be designed using three binary inputs?\n",
        "# Find respective weights w1 to w8) and demonstrate for all the possible boolean functions for three inputs.\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# -----------------------------\n",
        "# 3-input combinations\n",
        "# -----------------------------\n",
        "X = np.array([\n",
        "    [0, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 0],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 0],\n",
        "    [1, 1, 1]\n",
        "])\n",
        "\n",
        "# Step activation\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Single perceptron training\n",
        "def train_perceptron(X, y, lr=0.1, epochs=50):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        errors = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = step(np.dot(X[i], w) + b)\n",
        "            error = y[i] - y_pred\n",
        "            if error != 0:\n",
        "                w += lr * error * X[i]\n",
        "                b += lr * error\n",
        "                errors += 1\n",
        "        if errors == 0:\n",
        "            return True   # Linearly separable\n",
        "    return False          # Not linearly separable\n",
        "\n",
        "# -----------------------------\n",
        "# Generate all Boolean functions\n",
        "# -----------------------------\n",
        "boolean_functions = list(itertools.product([0, 1], repeat=8))\n",
        "\n",
        "separable = 0\n",
        "non_separable = 0\n",
        "\n",
        "for func in boolean_functions:\n",
        "    y = np.array(func)\n",
        "    if train_perceptron(X, y):\n",
        "        separable += 1\n",
        "    else:\n",
        "        non_separable += 1\n",
        "\n",
        "# -----------------------------\n",
        "# Results\n",
        "# -----------------------------\n",
        "print(\"Total Boolean functions (3 inputs):\", len(boolean_functions))\n",
        "print(\"Linearly separable functions:\", separable)\n",
        "print(\"Non-linearly separable functions:\", non_separable)\n"
      ],
      "metadata": {
        "id": "4G1mL-ipQ9sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ffb1bb-519a-47e9-a2e7-9d9e394bac61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Boolean functions (3 inputs): 256\n",
            "Linearly separable functions: 104\n",
            "Non-linearly separable functions: 152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 3\n"
      ],
      "metadata": {
        "id": "kMHxSppRsh56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#9) Implement an MLP by varying bias, weights, and learning rate, and record observations for different learning rate values.\n",
        "# Plot a graph showing the relationship between loss (error) and learning rate."
      ],
      "metadata": {
        "id": "5EFJe-TK4NAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}